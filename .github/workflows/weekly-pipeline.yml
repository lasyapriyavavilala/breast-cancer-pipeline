name: Weekly Breast Cancer News Pipeline - TEST

on:
  schedule:
    - cron: '0 9 * * 0'
  workflow_dispatch:

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Setup Chrome and ChromeDriver
        uses: browser-actions/setup-chrome@latest
        with:
          chrome-version: stable
      
      - name: Install ChromeDriver
        run: |
          which chromedriver
          chromedriver --version
      
      - name: Create directories
        run: |
          mkdir -p data/raw data/processed data/outputs data/embeddings logs
      
      - name: Copy CSV files to data directory
        run: |
          echo "Copying CSV files..."
          cp pharma_urls.csv data/pharma_urls.csv
          cp keywords.csv data/keywords.csv
          echo "Verifying files:"
          ls -lh data/*.csv
      
      - name: Run pipeline (TEST MODE - 10 articles, 2 polls each)
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          TWITTER_API_KEY: ${{ secrets.TWITTER_API_KEY }}
          TWITTER_API_SECRET: ${{ secrets.TWITTER_API_SECRET }}
          TWITTER_ACCESS_TOKEN: ${{ secrets.TWITTER_ACCESS_TOKEN }}
          TWITTER_ACCESS_TOKEN_SECRET: ${{ secrets.TWITTER_ACCESS_TOKEN_SECRET }}
          TWITTER_BEARER_TOKEN: ${{ secrets.TWITTER_BEARER_TOKEN }}
        run: |
          python run_pipeline.py \
            --target 10 \
            --days-back 7 \
            --polls-per-article 2 \
            --post-polls \
            --post-limit 5 \
            --no-dry-run \
            2>&1 | tee logs/pipeline_$(date +%Y%m%d_%H%M%S).log
      
      # ===== UPLOAD AGENT 1 OUTPUT =====
      - name: Upload Agent 1 - Scraped Articles
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: 01-agent1-scraped-articles
          path: data/raw/scraped_articles_*.json
          retention-days: 30
      
      # ===== UPLOAD AGENT 2 OUTPUT =====
      - name: Upload Agent 2 - Enhanced Articles (with entities)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: 02-agent2-enhanced-articles
          path: |
            data/processed/enhanced_articles.json
            data/processed/enhanced_articles.ndjson
          retention-days: 30
      
      # ===== UPLOAD AGENT 3 OUTPUT =====
      - name: Upload Agent 3 - Categorized Articles
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: 03-agent3-categorized-articles
          path: data/processed/categorized_articles.json
          retention-days: 30
      
      # ===== UPLOAD AGENT 4 OUTPUT =====
      - name: Upload Agent 4 - Generated Polls
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: 04-agent4-generated-polls
          path: data/outputs/twitter_polls.json
          retention-days: 30
      
      # ===== UPLOAD AGENT 5 OUTPUT =====
      - name: Upload Agent 5 - Database (posted polls tracker)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: 05-agent5-database
          path: data/pharma_news.db
          retention-days: 30
      
      # ===== UPLOAD ALL LOGS =====
      - name: Upload Complete Pipeline Logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: 00-complete-logs
          path: logs/*.log
          retention-days: 30